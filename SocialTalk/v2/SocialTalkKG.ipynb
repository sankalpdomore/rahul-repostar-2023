{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.datasets.inductive.ilp_teru import InductiveFB15k237\n",
    "from pykeen.datasets.inductive.base import DisjointInductivePathDataset\n",
    "\n",
    "from pykeen.models.inductive import InductiveNodePieceGNN\n",
    "\n",
    "from pykeen.models import predict\n",
    "from pykeen.training import SLCWATrainingLoop\n",
    "from pykeen.evaluation.rank_based_evaluator import SampledRankBasedEvaluator\n",
    "from pykeen.stoppers import EarlyStopper\n",
    "from pykeen.losses import NSSALoss\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = '../data/SocialTalk'\n",
    "\n",
    "TRAIN_URL = f'{BASE_URL}/training/train.txt'\n",
    "INDUCTIVE_INFERENCE_URL = f'{BASE_URL}/inference/train.txt'\n",
    "INDUCTIVE_VALIDATION_URL = f'{BASE_URL}/inference/valid.txt'\n",
    "INDUCTIVE_TEST_URL = f'{BASE_URL}/inference/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InductiveClientData(DisjointInductivePathDataset):\n",
    "    def __init__(self, create_inverse_triples: bool = False, **kwargs):\n",
    "        \"\"\"Initialize client data from triples file path\"\"\"\n",
    "        super().__init__(\n",
    "            transductive_training_path=TRAIN_URL,\n",
    "            inductive_inference_path=INDUCTIVE_INFERENCE_URL,\n",
    "            inductive_validation_path=INDUCTIVE_VALIDATION_URL,\n",
    "            inductive_testing_path=INDUCTIVE_TEST_URL,\n",
    "            create_inverse_triples=create_inverse_triples,\n",
    "            eager=True,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 15 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 15 from 16084 triples were filtered out\n",
      "You're trying to map triples with 9 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 9 from 8042 triples were filtered out\n"
     ]
    }
   ],
   "source": [
    "# dataset = InductiveFB15k237(version=\"v4\", create_inverse_triples=True)\n",
    "dataset = InductiveClientData(create_inverse_triples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blake/miniconda3/lib/python3.8/site-packages/pykeen/nn/representation.py:373: UserWarning: Directly use Embedding.max_id instead of num_embeddings.\n",
      "  warnings.warn(f\"Directly use {self.__class__.__name__}.max_id instead of num_embeddings.\")\n",
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "model = InductiveNodePieceGNN(\n",
    "    triples_factory=dataset.transductive_training,  # training factory, will be also used for a GNN\n",
    "    inference_factory=dataset.inductive_inference,  # inference factory, will be used for a GNN\n",
    "    num_tokens=12,  # length of a node hash - how many unique relations per node will be used\n",
    "    aggregation=\"mlp\",  # aggregation function, defaults to an MLP, can be any PyTorch function\n",
    "    loss=NSSALoss(margin=15),  # dummy loss\n",
    "    random_seed=42,\n",
    "    gnn_encoder=None,  # defaults to a 2-layer CompGCN with DistMult composition function\n",
    ")\n",
    "optimizer = Adam(params=model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop = SLCWATrainingLoop(\n",
    "    triples_factory=dataset.transductive_training,  # training triples\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    negative_sampler_kwargs=dict(num_negs_per_pos=32),\n",
    "    mode=\"training\",   # necessary to specify for the inductive mode - training has its own set of nodes\n",
    ")\n",
    "\n",
    "# Validation and Test evaluators use a restricted protocol ranking against 50 random negatives\n",
    "valid_evaluator = SampledRankBasedEvaluator(\n",
    "    mode=\"validation\",   # necessary to specify for the inductive mode - this will use inference nodes\n",
    "    evaluation_factory=dataset.inductive_validation,  # validation triples to predict\n",
    "    additional_filter_triples=dataset.inductive_inference.mapped_triples,   # filter out true inference triples\n",
    ")\n",
    "\n",
    "# According to the original code\n",
    "# https://github.com/kkteru/grail/blob/2a3dffa719518e7e6250e355a2fb37cd932de91e/test_ranking.py#L526-L529\n",
    "# test filtering uses only the inductive_inference split and does not include inductive_validation triples\n",
    "# If you use the full RankBasedEvaluator, both inductive_inference and inductive_validation triples\n",
    "# must be added to the additional_filter_triples\n",
    "test_evaluator = SampledRankBasedEvaluator(\n",
    "    mode=\"testing\",   # necessary to specify for the inductive mode - this will use inference nodes\n",
    "    evaluation_factory=dataset.inductive_testing,  # test triples to predict\n",
    "    additional_filter_triples=dataset.inductive_inference.mapped_triples,   # filter out true inference triples\n",
    ")\n",
    "\n",
    "early_stopper = EarlyStopper(\n",
    "    model=model,\n",
    "    training_triples_factory=dataset.inductive_inference,\n",
    "    evaluation_triples_factory=dataset.inductive_validation,\n",
    "    frequency=1,\n",
    "    patience=10000,  # for test reasons, turn it off\n",
    "    result_tracker=None,\n",
    "    evaluation_batch_size=256,\n",
    "    evaluator=valid_evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu: 100%|██████████| 100/100 [2:13:25<00:00, 80.06s/epoch, loss=0.000162, prev_loss=0.000162] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0008836611462998171,\n",
       " 0.0006925249483855627,\n",
       " 0.00042594519832943274,\n",
       " 0.00028770030688344173,\n",
       " 0.0002420711339997736,\n",
       " 0.00021706705763834005,\n",
       " 0.00020351311368394672,\n",
       " 0.00019560123517120488,\n",
       " 0.00018845166221624648,\n",
       " 0.00018524378642488078,\n",
       " 0.0001822579473505665,\n",
       " 0.00017933882961923488,\n",
       " 0.0001771795113855765,\n",
       " 0.00017653864028490708,\n",
       " 0.00017603478839470674,\n",
       " 0.00017394494767156076,\n",
       " 0.00017303186589369616,\n",
       " 0.00017116162997848574,\n",
       " 0.00017160748900795555,\n",
       " 0.00017197314593931696,\n",
       " 0.00017116766067479637,\n",
       " 0.0001711466093292549,\n",
       " 0.00016993859831978422,\n",
       " 0.0001684870130050393,\n",
       " 0.00017005952751816833,\n",
       " 0.0001687661536985014,\n",
       " 0.00016803606700864564,\n",
       " 0.0001673643835319347,\n",
       " 0.00016759869220956002,\n",
       " 0.00016666413568919414,\n",
       " 0.0001671467188827367,\n",
       " 0.0001666981819530103,\n",
       " 0.00016750877772261244,\n",
       " 0.0001669634851665084,\n",
       " 0.00016658228487648308,\n",
       " 0.0001655656445388352,\n",
       " 0.00016657433025464712,\n",
       " 0.00016659135872032493,\n",
       " 0.00016523730695067694,\n",
       " 0.00016535682385959902,\n",
       " 0.00016559625260830594,\n",
       " 0.00016584037577008003,\n",
       " 0.00016500870111645143,\n",
       " 0.0001661048869767497,\n",
       " 0.0001652268051059427,\n",
       " 0.00016533629568729667,\n",
       " 0.00016488274918828323,\n",
       " 0.00016601671118278102,\n",
       " 0.00016436374034784526,\n",
       " 0.00016446970281445335,\n",
       " 0.00016526386628309598,\n",
       " 0.00016475386913648743,\n",
       " 0.00016408445958495215,\n",
       " 0.0001646939833092863,\n",
       " 0.00016486308392716118,\n",
       " 0.0001639876750286713,\n",
       " 0.00016418613596175577,\n",
       " 0.00016489076665584073,\n",
       " 0.00016312486084179946,\n",
       " 0.00016378895848047034,\n",
       " 0.00016379586188586435,\n",
       " 0.00016246987102076457,\n",
       " 0.00016304684958960564,\n",
       " 0.0001635672467755176,\n",
       " 0.00016311468174810934,\n",
       " 0.0001628663650838161,\n",
       " 0.00016211880844288628,\n",
       " 0.00016247023348520283,\n",
       " 0.00016365426804024384,\n",
       " 0.00016308242623950262,\n",
       " 0.00016281677946166333,\n",
       " 0.00016374824355510378,\n",
       " 0.00016347395334544913,\n",
       " 0.0001632913982934426,\n",
       " 0.0001618775739812244,\n",
       " 0.00016329898755205238,\n",
       " 0.00016276467003881843,\n",
       " 0.00016251941698749894,\n",
       " 0.00016317422772141774,\n",
       " 0.00016292115890020244,\n",
       " 0.00016246545135481394,\n",
       " 0.00016163395474473778,\n",
       " 0.00016234989430610668,\n",
       " 0.00016231313674799742,\n",
       " 0.00016301795331236213,\n",
       " 0.00016281556840605691,\n",
       " 0.00016262247179592198,\n",
       " 0.00016138904931606258,\n",
       " 0.00016386408810559438,\n",
       " 0.0001620148632427737,\n",
       " 0.00016251132774595318,\n",
       " 0.0001615580793774401,\n",
       " 0.0001625625795913875,\n",
       " 0.00016253612461533264,\n",
       " 0.0001609497017758635,\n",
       " 0.00016156170581907138,\n",
       " 0.00016176560725525126,\n",
       " 0.00016125288884251925,\n",
       " 0.0001618297709396992,\n",
       " 0.00016161040887626192]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training starts here\n",
    "training_loop.train(\n",
    "    triples_factory=dataset.transductive_training,\n",
    "    stopper=early_stopper,\n",
    "    num_epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on cpu: 100%|██████████| 8.03k/8.03k [00:03<00:00, 2.25ktriple/s]\n"
     ]
    }
   ],
   "source": [
    "# Test evaluation\n",
    "result = test_evaluator.evaluate(\n",
    "    model=model,\n",
    "    mapped_triples=dataset.inductive_testing.mapped_triples,\n",
    "    additional_filter_triples=dataset.inductive_inference.mapped_triples,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6296526826839288"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get_metric('hits@10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../data/SocialTalk/kg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_entity_representations(inference=False):\n",
    "    if inference:\n",
    "        entity_representation_modules = model.inference_representation\n",
    "        index = dataset.inductive_inference.entity_id_to_label.values()\n",
    "    else:\n",
    "        entity_representation_modules = model.entity_representations\n",
    "        index = dataset.transductive_training.entity_id_to_label.values()\n",
    "\n",
    "    entity_embeddings = entity_representation_modules[0]\n",
    "    entity_embedding_tensor = entity_embeddings()\n",
    "    return pd.DataFrame(data=entity_embedding_tensor.detach().numpy(), index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_example_entity_representations(inference=True)\n",
    "embeddings.to_csv('../data/SocialTalk/inference/embeddings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference from saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blake/miniconda3/lib/python3.8/site-packages/pykeen/nn/representation.py:373: UserWarning: Directly use Embedding.max_id instead of num_embeddings.\n",
      "  warnings.warn(f\"Directly use {self.__class__.__name__}.max_id instead of num_embeddings.\")\n",
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "model2 = InductiveNodePieceGNN(\n",
    "    triples_factory=dataset.transductive_training,  # training factory, will be also used for a GNN\n",
    "    inference_factory=dataset.inductive_inference,  # inference factory, will be used for a GNN\n",
    "    num_tokens=12,  # length of a node hash - how many unique relations per node will be used\n",
    "    aggregation=\"mlp\",  # aggregation function, defaults to an MLP, can be any PyTorch function\n",
    "    loss=NSSALoss(margin=15),  # dummy loss\n",
    "    random_seed=42,\n",
    "    gnn_encoder=None,  # defaults to a 2-layer CompGCN with DistMult composition function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load('../data/SocialTalk/kg.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_example_entity_representations(inference=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_profiles = pd.read_csv('../data/SocialTalk/Apres-profiles-SocialTalk.csv').set_index('Username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lookup = pd.read_csv('../data/SocialTalk/audience_populated.csv')['Username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_25_profile_interests = desired_profiles.iloc[:, 32:57]  # INCLUDE (list of items per user)\n",
    "top_25_audience_report = desired_profiles.iloc[:, 191:]  # INCLUDE (tuples w/ percentages)\n",
    "age_distribution = desired_profiles.iloc[:, 183:189]  # INCLUDE (list of percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = top_25_profile_interests.unstack()\n",
    "x = x[x.notnull()].reset_index().drop('level_0', axis=1)\n",
    "x.columns = ['user_id', 'interest_category']\n",
    "x.to_csv('../data/SocialTalk/predict/interest_categories.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = top_25_audience_report[[i for i in top_25_audience_report if i[-1] == '%']]\n",
    "x2 = top_25_audience_report[[i for i in top_25_audience_report if i[-1] != '%']]\n",
    "\n",
    "filter_audiences = x1 >= 0.19\n",
    "filter_audiences.columns = [i[:-2] for i in filter_audiences.columns]\n",
    "\n",
    "x3 = x2[filter_audiences].unstack()\n",
    "x3 = x3[x3.notnull()].reset_index().drop('level_0', axis=1)\n",
    "x3.columns = ['user_id', 'audience_category']\n",
    "x3.to_csv('../data/SocialTalk/predict/audience_categories.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ages = age_distribution >= 0.17\n",
    "x = age_distribution[filter_ages]\n",
    "x.columns = [i[:-2] for i in x.columns]\n",
    "x = x.unstack()\n",
    "x = x[x.notnull()].reset_index().drop(0, axis=1)\n",
    "x.columns = ['age_category', 'user_id']\n",
    "x.to_csv('../data/SocialTalk/predict/age_categories.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e535d220b488de5ea91b9cc2b59c2f114a95afd4d79a4d51c41214c2f625fe35"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
